{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f750601-9c17-4063-a043-47c4d58b509b",
   "metadata": {},
   "source": [
    "# Machine learning example with OSTIA\n",
    "\n",
    "This is based off ERA example, and isn't working yet: just a proof of concept of loading/using data, the results are garbage.\n",
    "\n",
    "There is no download script yet, the data can be downloaded manually from https://data.marine.copernicus.eu/product/SST_GLO_SST_L4_NRT_OBSERVATIONS_010_001/files?subdataset=METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2, or using copernicus API (not tested yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c4f3c5-1d81-4f69-8f10-4006b7c20f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cabe45-8819-4348-9194-90382a5633ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine different dates in one dataset\n",
    "path = \"./data\"\n",
    "nc_files = glob.glob(os.path.join(path, \"*.nc\"))\n",
    "\n",
    "datasets = [xr.open_dataset(f, engine='netcdf4') for f in nc_files]\n",
    "\n",
    "combined_ds = xr.concat(datasets, dim='time')\n",
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c8b78-80fc-4745-947b-20e8aa46c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset original dataset to one degree to save space\n",
    "subset_ds = combined_ds.isel(lat=slice(None, None, 20), lon=slice(None, None, 20))\n",
    "subset_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7553f1-c2d3-476b-82ac-35e2752742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample SST\n",
    "fig, ax = plt.subplots(figsize=(8, 5),\n",
    "                       subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "pcolor_plot = ax.pcolormesh(subset_ds[\"lon\"],\n",
    "                            subset_ds[\"lat\"],\n",
    "                            subset_ds[\"analysed_sst\"][5,:,:],\n",
    "                            transform=ccrs.PlateCarree(),\n",
    "                            cmap='viridis')\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "cbar = fig.colorbar(pcolor_plot, ax=ax, shrink=0.75, orientation='horizontal')\n",
    "ax.set_title('Subsetted SST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56270a-1765-44e0-a6a4-13eebb157a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample ice concentration\n",
    "fig, ax = plt.subplots(figsize=(8, 5),\n",
    "                       subplot_kw={'projection': ccrs.NorthPolarStereo()})\n",
    "\n",
    "pcolor_plot = ax.pcolormesh(subset_ds[\"lon\"],\n",
    "                            subset_ds[\"lat\"],\n",
    "                            subset_ds[\"sea_ice_fraction\"][5,:,:],\n",
    "                            transform=ccrs.PlateCarree(),\n",
    "                            cmap='jet')\n",
    "ax.coastlines()\n",
    "ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "ax.set_extent([-180, 180, 60, 90], ccrs.PlateCarree())\n",
    "cbar = fig.colorbar(pcolor_plot, ax=ax, shrink=0.75, orientation='horizontal')\n",
    "ax.set_title('Subsetted sea ice fraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1da15-c059-42c6-8bd7-54685fd8725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyper parameters\n",
    "config = {\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 1e-3,\n",
    "    'test_size': 0.2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b601b4-3f54-47e6-9674-39e31a269d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to use (ignore the mask and analysis error)\n",
    "data_vars = ['analysed_sst', 'sea_ice_fraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0da84-bc11-4e8c-84fc-7e2804287e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Extract variables from dataset and stack them into a numpy array (time, lat, long, vars)\n",
    "data = np.stack([subset_ds[var].values for var in data_vars], axis=-1)\n",
    "\n",
    "# Reshape the data for StandardScaler (it expects 2D, so combine lat, lon, and vars)\n",
    "# Reshape to (time, lat*lon*vars) for scaling, later we'll reshape back\n",
    "n_time, n_lat, n_lon, n_vars = data.shape\n",
    "reshaped_data = data.reshape(n_time, -1)\n",
    "\n",
    "# Ignore nans when standardizing\n",
    "# AS: this is added compared to the ERA example and hasn't been properly debugged yet\n",
    "nan_mask = np.isnan(reshaped_data[0,:])\n",
    "reshaped_data_clean = reshaped_data[:,~nan_mask]\n",
    "\n",
    "# Fit and transform the data using StandardScaler\n",
    "scaled_data_clean = scaler.fit_transform(reshaped_data_clean)\n",
    "\n",
    "# Put rescaled data back with nans\n",
    "scaled_data = reshaped_data.copy()\n",
    "scaled_data[:,~nan_mask] = scaled_data_clean\n",
    "# put zeros where there were nans\n",
    "scaled_data[:,nan_mask] = 0.0\n",
    "\n",
    "# Reshape back to original (time, lat, long, vars) shape\n",
    "standardized_data = scaled_data.reshape(n_time, n_lat, n_lon, n_vars)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test = train_test_split(standardized_data, test_size=config['test_size'])\n",
    "\n",
    "# Convert to PyTorch tensors and change dimensions to (time, vars, lat, long)\n",
    "tensor_data_train = torch.Tensor(X_train).permute(0, 3, 1, 2)\n",
    "tensor_data_test = torch.Tensor(X_test).permute(0, 3, 1, 2)\n",
    "\n",
    "# Create TensorDataset\n",
    "tensor_dataset_train = TensorDataset(tensor_data_train)\n",
    "tensor_dataset_test = TensorDataset(tensor_data_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(tensor_dataset_train, batch_size=config['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(tensor_dataset_test, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ae07c-1373-4e95-8eff-2c9b81bed08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device to use\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "# Define the model\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_size[0], 16, kernel_size=3, stride=(1,2), padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  \n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, input_size[0], kernel_size=3, stride=(1,2), padding=1, output_padding=(0, 1)),  \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "dummy_input= next(iter(test_loader))[0].to(device)\n",
    "model = ConvAutoencoder(dummy_input[0].shape).to(device)\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Output shape:\", model(dummy_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c9310-f7be-48c8-ade6-6c03f9806230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for reconstruction\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "num_epochs = config['num_epochs']\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    train_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{config['num_epochs']-1}\", leave=False)\n",
    "    for data in progress_bar:\n",
    "        inputs = data[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)  # Reconstruction loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix({'Train Loss': loss.item()})\n",
    "\n",
    "    # Calculate average training loss\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    model.eval()  \n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():  \n",
    "        for data in test_loader:\n",
    "            inputs = data[0].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    # Calculate average test loss\n",
    "    test_loss /= len(test_loader)\n",
    "\n",
    "    # Write test loss to tensorboard\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{num_epochs-1}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ebd242-2f41-4e8c-8187-6efc9b36e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on one example\n",
    "test_data = next(iter(test_loader))[0].to(device)\n",
    "\n",
    "# Set the model in evaluation mode and turn off gradient calculations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Pass the test data through the model to get the reconstruction\n",
    "    reconstructed_data = model(test_data)\n",
    "\n",
    "def data2xarray(dataset, data):\n",
    "    for i, var in enumerate(data_vars):\n",
    "        getattr(dataset, var).values = data[i]\n",
    "    return dataset\n",
    "\n",
    "# Create a figure with Cartopy projections for both subplots\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 12),\n",
    "                         subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# Create the original and reconstructed xarray datasets\n",
    "original_data_xr = data2xarray(subset_ds.isel(time=0).copy(), test_data[0].to('cpu'))\n",
    "reconstructed_data_xr = data2xarray(subset_ds.isel(time=0).copy(), reconstructed_data[0].to('cpu'))\n",
    "\n",
    "# Plot original data with Cartopy features\n",
    "original_data_xr.analysed_sst.plot(ax=axes[0], transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "axes[0].coastlines()\n",
    "axes[0].gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "axes[0].set_title('Original Data')\n",
    "\n",
    "# Plot reconstructed data with Cartopy features\n",
    "reconstructed_data_xr.analysed_sst.plot(ax=axes[1], transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "axes[1].coastlines()\n",
    "axes[1].gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)\n",
    "axes[1].set_title('Reconstructed Data')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_readiness",
   "language": "python",
   "name": "data_readiness"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
